# tokn

A visual flow-based IDE for prompt engineering, testing, and optimization with Large Language Models.

![Demo](assets/dspy.png)

## Overview

tokn is a web application that provides a node-based interface for building, testing, and optimizing prompts across multiple AI model providers. Design complex LLM workflows visually, test prompts with different models, and automatically optimize them using evolutionary algorithms.

## ğŸš€ Quick Start

**New to tokn?** Check out the [Quick Start Guide](QUICKSTART.md) to get running in 3 minutes!

```bash
npm install
npm run dev
# Open http://localhost:3000 and add your API keys in Settings
```

## Features

### Visual Flow Editor
- Drag-and-drop node-based canvas with pan and zoom
- Connect nodes to build multi-step LLM workflows
- Real-time execution status indicators
- Multi-node selection and manipulation

### Node Types
- **Prompt Node** - Define system and user prompts
- **Model Node** - Execute LLM inference with configurable parameters
- **Tool Node** - Create custom JavaScript tools for LLMs to call
- **DSPy Node** - Optimize prompts using the DSPy framework with automatic few-shot learning
- **MLflow Node** - Optimize prompts using evolutionary algorithms with MLflow experiment tracking

### Multi-Provider Support
- **OpenAI** - GPT-4, GPT-4o, GPT-3.5 models via API
- **Claude (Anthropic)** - Claude 3.5 Sonnet, Claude 3 Opus, Haiku models
- **Gemini (Google)** - Gemini 2.0 Flash, Gemini 1.5 Pro models
- Provider-agnostic architecture for easy extensibility

### Tool Calling System
- Define custom tools with JSON schemas
- JavaScript-based tool implementation using browser APIs
- Sandboxed execution in Web Workers (no Node.js modules)
- 30-second timeout and 5MB output limits for security
- Automatic tool registration with compatible models

### DSPy Integration
- Prompt optimization using the DSPy framework
- Automatic few-shot example selection
- Signature-based prompt compilation
- Test dataset evaluation with real-time progress tracking

### MLflow Integration
- Evolutionary prompt optimization using MLflow's GEPA library
- Multi-provider model ensemble for optimization
- Experiment tracking and metrics visualization
- Population-based search with configurable mutation and elite selection

### Workflow Management
- Save and load workflows (.toknflow files)
- Auto-save functionality with recovery
- Keyboard shortcuts (Ctrl+S, Ctrl+O, Ctrl+N)

### Security
- Encrypted API key storage using Web Crypto API (AES-GCM 256-bit)
- Keys stored in browser IndexedDB, never in plaintext
- Sandboxed tool execution in Web Workers
- Optional backend for DSPy/GEPA (API keys transmitted over HTTPS only)

## ğŸ³ Docker Deployment

tokn can be easily deployed using Docker and Docker Compose for both development and production environments.

### Quick Start with Docker

**Development mode (with hot reloading):**
```bash
# Frontend only (recommended for most users)
docker-compose --profile frontend up
# or: make dev-frontend

# Full stack (if using DSPy/GEPA optimization)
docker-compose --profile full up
# or: make dev

# Access the app at http://localhost:3000
```

**Production mode:**
```bash
# Frontend only
docker-compose -f docker-compose.prod.yml --profile frontend up -d
# or: make prod-frontend

# Full stack (if using optimization)
docker-compose -f docker-compose.prod.yml --profile full up -d
# or: make prod

# Access the app at http://localhost:80
```

### Services

The Docker setup includes:

- **Frontend** - React app with Vite (dev) or Nginx (prod) on port 3000/80
- **Backend** (optional) - Python Flask API for DSPy/GEPA optimization on port 5000

**Note:** Backend is only needed if you're using the DSPy or GEPA optimization features. Most users can run just the frontend!

### Documentation

- **[DOCKER.md](DOCKER.md)** - Complete Docker usage guide
- **[DOCKER_DEPLOYMENT.md](DOCKER_DEPLOYMENT.md)** - Deployment to production

## File Structure

```
tokn/
â”œâ”€â”€ src/                     # React application source
â”‚   â”œâ”€â”€ main.jsx             # React entry point
â”‚   â”œâ”€â”€ App.jsx              # Main React component
â”‚   â”œâ”€â”€ services/            # Core services
â”‚   â”‚   â”œâ”€â”€ webStorage.js    # Encrypted API key storage (Web Crypto + IndexedDB)
â”‚   â”‚   â””â”€â”€ fileOperations.js # File System Access API wrapper
â”‚   â””â”€â”€ workers/             # Web Workers
â”‚       â””â”€â”€ toolWorker.js    # Sandboxed tool execution
â”œâ”€â”€ renderer/                # Original application logic (transitioning)
â”‚   â”œâ”€â”€ script.js            # Core app logic and state
â”‚   â”œâ”€â”€ main.css             # Application styles
â”‚   â”œâ”€â”€ model-adapters.js    # Provider-specific adapters (OpenAI, Claude, Gemini)
â”‚   â”œâ”€â”€ dspy-worker.js       # DSPy backend API client
â”‚   â”œâ”€â”€ gepa-worker.js       # GEPA backend API client
â”‚   â”œâ”€â”€ tool-script.js       # Tool node implementation
â”‚   â””â”€â”€ tool-worker-launcher.js # Web Worker launcher
â”œâ”€â”€ backend/                 # Python Flask API (optional)
â”‚   â”œâ”€â”€ app.py               # Flask server
â”‚   â”œâ”€â”€ routes/              # API endpoints
â”‚   â”‚   â”œâ”€â”€ dspy_route.py    # DSPy optimization endpoint
â”‚   â”‚   â””â”€â”€ gepa_route.py    # GEPA optimization endpoint
â”‚   â”œâ”€â”€ dspy/                # DSPy Python scripts
â”‚   â””â”€â”€ gepa/                # GEPA Python scripts
â”œâ”€â”€ services/                # Business logic
â”‚   â”œâ”€â”€ providerRegistry.js  # Provider management
â”‚   â””â”€â”€ config.js            # Configuration
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ BACKEND_API.md       # Backend API documentation
â”‚   â””â”€â”€ DEPLOYMENT.md        # Deployment guide
â”œâ”€â”€ index.html               # Vite entry point
â”œâ”€â”€ vite.config.js           # Vite configuration
â””â”€â”€ package.json             # Dependencies and metadata
```

### Usage

1. **Create a Workflow**
   - Add nodes from the top toolbar
   - Connect nodes by dragging from output to input ports
   - Configure nodes using the inspector panel

2. **Run a Prompt**
   - Connect a Prompt node to a Model node
   - Set your prompt text and model parameters
   - Click "Run Flow" to execute

3. **Optimize Prompts with DSPy**
   - Add a DSPy node
   - Define test cases and input/output signatures
   - Configure optimization parameters
   - Run to automatically improve your prompt with few-shot learning

4. **Optimize Prompts with MLflow**
   - Add an MLflow GEPA node
   - Define test cases with expected outputs
   - Configure evolutionary algorithm parameters (population size, generations, mutation rate)
   - Run to optimize your prompt using population-based search with MLflow tracking

## Use Cases

- Iterative prompt refinement with visual feedback
- Model comparison across providers
- Complex multi-step LLM workflows
- Tool development and testing for LLM function calling
- Automated prompt optimization
